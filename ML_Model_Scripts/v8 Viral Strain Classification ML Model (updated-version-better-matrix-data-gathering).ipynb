{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxTgm86BC9mC"
      },
      "outputs": [],
      "source": [
        "!pip install -U pip wheel setuptools\n",
        "!pip install concrete-ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yQEwnwKEQ3F"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhLWVwaGFNWw",
        "outputId": "c981ddae-5ea6-4362-b585-4bab7677204f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/vivs/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['B.1.1.529' 'B.1.617.2' 'B.1.621' 'C.37']\n",
            "Shape of x:  (8893, 512)\n",
            "Shape of y: (8893,)\n",
            "Running time is 0.37547874450683594 seconds\n"
          ]
        }
      ],
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score\n",
        "from concrete.ml.sklearn import LogisticRegression, LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression as skLR\n",
        "from sklearn.linear_model import LinearRegression as skLinear\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import time, numpy\n",
        "import pandas as pd\n",
        "\n",
        "start_time = time.time()\n",
        "def feature_selection(x, opt = \"\", var_percent = 0.8, n_features = 30):\n",
        "    if opt == \"var\":\n",
        "        print(\"\\nUsing variance threshold feature selection...\")\n",
        "        # Remove all features that have low variance in more than (percent)% of the samples.\n",
        "        #percent = .80\n",
        "        sel = VarianceThreshold(threshold=(var_percent * (1 - var_percent)))\n",
        "        print(\"Shape of X: \", x.shape)\n",
        "        #print(x.iloc[0])\n",
        "        x = sel.fit_transform(x)\n",
        "        print(\"After feature selection: \", len(x[0]), \" features\")\n",
        "        #print(x[0])\n",
        "    elif opt == \"kbest\":\n",
        "        print(\"\\nUsing K best features feature selection...\")\n",
        "        print(\"Shape of x before selection: \", x.shape)\n",
        "        x = SelectKBest(chi2, k=n_features).fit_transform(x, y)\n",
        "        print(\"Shape of x after selection: \", x.shape)\n",
        "    elif opt == \"pca\":\n",
        "        print(\"\\nUsing PCA feature selection...\")\n",
        "        x_scaled = StandardScaler().fit_transform(x)\n",
        "        pca = PCA(n_components=n_features)\n",
        "        pca_features = pca.fit_transform(x_scaled)\n",
        "        print('Shape before PCA: ', x_scaled.shape)\n",
        "        print('Shape after PCA: ', pca_features.shape)\n",
        "        x = pca_features\n",
        "    else:\n",
        "        print(\"\")\n",
        "    return x\n",
        "\n",
        "#select set option\n",
        "set = -1\n",
        "\n",
        "#import the dataset here (GISAID; try downloading individual FASTA files if di talaga kaya idownload yung EPISET)\n",
        "#if(set == -1):\n",
        "#    dataset_name = \"AFHE DATASET (05-04-2023).csv\"\n",
        "#elif(set == 0):\n",
        "#    dataset_name = \"ADJUSTED DATASET (05-04-2023).csv\"\n",
        "#else:\n",
        "#    dataset_name = \"ADJUSTED DOWNSIZED DATASET (05-04-2023).csv\"\n",
        "#dataset = pd.read_csv(dataset_name)\n",
        "\n",
        "dataset = pd.read_csv(\"AFHE DATASET (05-18-2023).csv\")\n",
        "\n",
        "#print(dataset['Lineage'].value_counts())\n",
        "\n",
        "#remove \"EPI_ISL_\" from ID\n",
        "#new_ID_col = []\n",
        "#for i in range(0, len(dataset['Accession ID'])):\n",
        "#    new_ID_col.append(str(dataset['Accession ID'].loc[i].replace(\"EPI_ISL_\", \"\")))\n",
        "#dataset['Accession ID'] = new_ID_col\n",
        "\n",
        "feature_cols = [c for c in dataset.columns[2:]]\n",
        "#print(feature_cols)\n",
        "\n",
        "x = dataset.loc[:,feature_cols].values #must be floats\n",
        "y = dataset.loc[:,'Lineage'].values #must be integers\n",
        "\n",
        "#print(x)\n",
        "#print(y)\n",
        "\n",
        "# Preprocessing with labels for the lineage\n",
        "le = preprocessing.LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "print(le.classes_)\n",
        "\n",
        "x = x.astype(float)\n",
        "\n",
        "print(\"Shape of x: \", x.shape)\n",
        "print(\"Shape of y:\", y.shape)\n",
        "\n",
        "print(f\"Running time is {time.time() - start_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZM8Rdc1GAiq",
        "outputId": "229d7be1-b1ad-4eb9-8d57-7e1ec6aebf07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Using K best features feature selection...\n",
            "Shape of x before selection:  (8893, 512)\n",
            "Shape of x after selection:  (8893, 20)\n",
            "Running time is 0.10820603370666504 seconds\n"
          ]
        }
      ],
      "source": [
        "# Feature Selection Cell (ensemble attempted)\n",
        "\n",
        "start_time = time.time()\n",
        "#x = feature_selection(x, \"var\", var_percent = 0.80)\n",
        "x = feature_selection(x, \"kbest\", n_features = 20)\n",
        "#x = feature_selection(x, \"pca\", n_features = 5)\n",
        "print(f\"Running time is {time.time() - start_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to8ualx7GEFT",
        "outputId": "3b2c82f9-f1c6-4422-9b21-a0799b6e2749"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set size: (1779, 20)\n",
            "Running time is 0.003861665725708008 seconds\n"
          ]
        }
      ],
      "source": [
        "# Retrieve train and test sets\n",
        "\n",
        "def split(x,y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=.20)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "start_time = time.time()\n",
        "X_train, X_test, y_train, y_test = split(x,y)\n",
        "\n",
        "print(f\"Test set size: {X_test.shape}\")\n",
        "\n",
        "print(f\"Running time is {time.time() - start_time} seconds\")\n",
        "#st_x = StandardScaler()\n",
        "#X_train = st_x.fit_transform(X_train)\n",
        "#X_test = st_x.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIUoad8WGHaf",
        "outputId": "4a30afab-a734-4593-d78a-40df98193cbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting performance metrics for scikit-learn model (Plaintext)...\n",
            "99.26925238898258\n",
            "99.38167509836987\n",
            "98.93198426082068\n",
            "99.32546374367622\n",
            "98.87577290612704\n",
            "99.15682967959528\n",
            "99.04440697020797\n",
            "99.15682967959528\n",
            "98.93198426082068\n",
            "99.15682967959528\n",
            "Getting performance metrics for Concrete-ML model (Quantized Plaintext)...\n",
            "99.21304103428893\n",
            "99.32546374367622\n",
            "99.21304103428893\n",
            "99.32546374367622\n",
            "99.21304103428893\n",
            "99.04440697020797\n",
            "99.10061832490163\n",
            "99.26925238898258\n",
            "99.04440697020797\n",
            "99.04440697020797\n",
            "Getting performance metrics for Concrete-ML model (FHE)...\n",
            "Compiling...\n",
            "model compiled!\n",
            "Compiling...\n",
            "model compiled!\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m     model\u001b[39m.\u001b[39mcompile(x)\n\u001b[1;32m     45\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmodel compiled!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m     y_pred_fhe \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_test, fhe\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mexecute\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     47\u001b[0m     fhe_results\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00maccuracy_score(y_test,\u001b[39m \u001b[39my_pred_fhe)\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39m\u001b[39m100\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m fhe_results:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concrete/ml/sklearn/base.py:719\u001b[0m, in \u001b[0;36mBaseClassifier.predict\u001b[0;34m(self, X, fhe)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: Data, fhe: Union[FheMode, \u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m FheMode\u001b[39m.\u001b[39mDISABLE) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m numpy\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    718\u001b[0m     \u001b[39m# Compute the predicted probabilities\u001b[39;00m\n\u001b[0;32m--> 719\u001b[0m     y_proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_proba(X, fhe\u001b[39m=\u001b[39;49mfhe)\n\u001b[1;32m    721\u001b[0m     \u001b[39m# Retrieve the class with the highest probability\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     y_preds \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39margmax(y_proba, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concrete/ml/sklearn/base.py:1612\u001b[0m, in \u001b[0;36mSklearnLinearClassifierMixin.predict_proba\u001b[0;34m(self, X, fhe)\u001b[0m\n\u001b[1;32m   1611\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_proba\u001b[39m(\u001b[39mself\u001b[39m, X: Data, fhe: Union[FheMode, \u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m FheMode\u001b[39m.\u001b[39mDISABLE) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m numpy\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m-> 1612\u001b[0m     y_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecision_function(X, fhe\u001b[39m=\u001b[39;49mfhe)\n\u001b[1;32m   1613\u001b[0m     y_proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_processing(y_logits)\n\u001b[1;32m   1614\u001b[0m     \u001b[39mreturn\u001b[39;00m y_proba\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concrete/ml/sklearn/base.py:1608\u001b[0m, in \u001b[0;36mSklearnLinearClassifierMixin.decision_function\u001b[0;34m(self, X, fhe)\u001b[0m\n\u001b[1;32m   1592\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Predict confidence scores.\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m \n\u001b[1;32m   1594\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[39m    numpy.ndarray: The predicted confidence scores.\u001b[39;00m\n\u001b[1;32m   1605\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1606\u001b[0m \u001b[39m# Here, we want to use SklearnLinearModelMixin's `predict` method as confidence scores are\u001b[39;00m\n\u001b[1;32m   1607\u001b[0m \u001b[39m# the dot product's output values, without any post-processing\u001b[39;00m\n\u001b[0;32m-> 1608\u001b[0m y_preds \u001b[39m=\u001b[39m SklearnLinearModelMixin\u001b[39m.\u001b[39;49mpredict(\u001b[39mself\u001b[39;49m, X, fhe\u001b[39m=\u001b[39;49mfhe)\n\u001b[1;32m   1609\u001b[0m \u001b[39mreturn\u001b[39;00m y_preds\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concrete/ml/sklearn/base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator.predict\u001b[0;34m(self, X, fhe)\u001b[0m\n\u001b[1;32m    615\u001b[0m     q_X_i \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mexpand_dims(q_X_i, \u001b[39m0\u001b[39m)\n\u001b[1;32m    617\u001b[0m     \u001b[39m# Disable mypy's union attribute error as we already check that fhe_circuit is not\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[39m# None using check_model_is_compiled\u001b[39;00m\n\u001b[1;32m    619\u001b[0m     q_y_pred_i \u001b[39m=\u001b[39m (\n\u001b[1;32m    620\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfhe_circuit\u001b[39m.\u001b[39msimulate(q_X_i)  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m    621\u001b[0m         \u001b[39mif\u001b[39;00m fhe \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msimulate\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 622\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfhe_circuit\u001b[39m.\u001b[39;49mencrypt_run_decrypt(q_X_i)  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     )\n\u001b[1;32m    624\u001b[0m     q_y_pred_list\u001b[39m.\u001b[39mappend(q_y_pred_i[\u001b[39m0\u001b[39m])\n\u001b[1;32m    626\u001b[0m q_y_pred \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(q_y_pred_list)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concrete/fhe/compilation/circuit.py:164\u001b[0m, in \u001b[0;36mCircuit.encrypt_run_decrypt\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencrypt_run_decrypt\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    152\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39m    Encrypt inputs, run the circuit, and decrypt the outputs in one go.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39m            clear result of homomorphic evaluation\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecrypt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencrypt(\u001b[39m*\u001b[39;49margs)))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concrete/fhe/compilation/circuit.py:115\u001b[0m, in \u001b[0;36mCircuit.encrypt\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencrypt\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Union[\u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mndarray]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PublicArguments:\n\u001b[1;32m    103\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m    Prepare inputs to be run on the circuit.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m            encrypted and plain arguments as well as public keys\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mencrypt(\u001b[39m*\u001b[39;49margs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concrete/fhe/compilation/client.py:184\u001b[0m, in \u001b[0;36mClient.encrypt\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeygen(force\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    182\u001b[0m keyset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys\u001b[39m.\u001b[39m_keyset  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m \u001b[39mreturn\u001b[39;00m ClientSupport\u001b[39m.\u001b[39;49mencrypt_arguments(\n\u001b[1;32m    185\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspecs\u001b[39m.\u001b[39;49mclient_parameters,\n\u001b[1;32m    186\u001b[0m     keyset,\n\u001b[1;32m    187\u001b[0m     [sanitized_args[i] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(sanitized_args))],\n\u001b[1;32m    188\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concrete/compiler/client_support.py:144\u001b[0m, in \u001b[0;36mClientSupport.encrypt_arguments\u001b[0;34m(client_parameters, keyset, args)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    136\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfunction has arity \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(signs)\u001b[39m}\u001b[39;00m\u001b[39m but is applied to too many arguments\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    139\u001b[0m lambda_arguments \u001b[39m=\u001b[39m [\n\u001b[1;32m    140\u001b[0m     ClientSupport\u001b[39m.\u001b[39m_create_lambda_argument(arg, signed)\n\u001b[1;32m    141\u001b[0m     \u001b[39mfor\u001b[39;00m arg, signed \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(args, signs)\n\u001b[1;32m    142\u001b[0m ]\n\u001b[1;32m    143\u001b[0m \u001b[39mreturn\u001b[39;00m PublicArguments\u001b[39m.\u001b[39mwrap(\n\u001b[0;32m--> 144\u001b[0m     _ClientSupport\u001b[39m.\u001b[39;49mencrypt_arguments(\n\u001b[1;32m    145\u001b[0m         client_parameters\u001b[39m.\u001b[39;49mcpp(),\n\u001b[1;32m    146\u001b[0m         keyset\u001b[39m.\u001b[39;49mcpp(),\n\u001b[1;32m    147\u001b[0m         [arg\u001b[39m.\u001b[39;49mcpp() \u001b[39mfor\u001b[39;49;00m arg \u001b[39min\u001b[39;49;00m lambda_arguments],\n\u001b[1;32m    148\u001b[0m     )\n\u001b[1;32m    149\u001b[0m )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "results = []\n",
        "print(\"Getting performance metrics for scikit-learn model (Plaintext)...\")\n",
        "\n",
        "for i in range(0,10):\n",
        "    X_train, X_test, y_train, y_test = split(x,y)\n",
        "    start_time = time.time()\n",
        "    skmodel = skLR(C=1)\n",
        "    skmodel.fit(X_train,y_train)\n",
        "    y_pred_sklearn = skmodel.predict(X_test)\n",
        "    results.append(f\"{skmodel.score(X_test,y_test)*100}\")\n",
        "\n",
        "for r in results:\n",
        "    print(f\"{r}\")\n",
        "#print(\"Accuracy for sklearn: \", skmodel.score(X_test,y_test)*100,\"%\")\n",
        "#print(f\"Running time is {time.time() - start_time} seconds\")\n",
        "\n",
        "quantized_results = []\n",
        "print(\"Getting performance metrics for Concrete-ML model (Quantized Plaintext)...\")\n",
        "for i in range(0,10):\n",
        "    X_train, X_test, y_train, y_test = split(x,y)\n",
        "    model = LogisticRegression(C=1)\n",
        "    # Fit the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    start_time = time.time()\n",
        "    # Run the predictions on non-encrypted data as a reference\n",
        "    y_pred_clear = model.predict(X_test)\n",
        "\n",
        "    quantized_results.append(f\"{model.score(X_test,y_test) * 100}\")\n",
        "    # print(\"Accuracy rate for quantized plaintext:  \", model.score(X_test,y_test) * 100, \"%\")\n",
        "    # print(f\"Running time is {time.time() - start_time} seconds\")\n",
        "\n",
        "for r in quantized_results:\n",
        "    print(f\"{r}\")\n",
        "\n",
        "fhe_results = []\n",
        "print(\"Getting performance metrics for Concrete-ML model (FHE)...\")\n",
        "for i in range(0,10):\n",
        "    X_train, X_test, y_train, y_test = split(x,y)\n",
        "    model = LogisticRegression(C=1)\n",
        "    #Fit the model\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Compiling...\")\n",
        "    model.compile(x)\n",
        "    print(\"model compiled!\")\n",
        "    y_pred_fhe = model.predict(X_test, fhe=\"execute\")\n",
        "    fhe_results.append(f\"{accuracy_score(y_test, y_pred_fhe) * 100}%\")\n",
        "\n",
        "for r in fhe_results:\n",
        "    print(f\"{r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "print(\"Getting performance metrics for scikit-learn model (Plaintext)...\")\n",
        "\n",
        "for i in range(0,10):\n",
        "    X_train, X_test, y_train, y_test = split(x,y)\n",
        "    start_time = time.time()\n",
        "    skmodel = skLR(C=1)\n",
        "    skmodel.fit(X_train,y_train)\n",
        "    y_pred_sklearn = skmodel.predict(X_test)\n",
        "    sklearn_recall = recall_score(y_test, y_pred_sklearn, average='weighted') * 100\n",
        "    results.append(f\"{sklearn_recall:.4f}\")\n",
        "\n",
        "for r in results:\n",
        "    print(f\"{r}\")\n",
        "#print(\"Accuracy for sklearn: \", skmodel.score(X_test,y_test)*100,\"%\")\n",
        "#print(f\"Running time is {time.time() - start_time} seconds\")\n",
        "\n",
        "quantized_results = []\n",
        "print(\"Getting performance metrics for Concrete-ML model (Quantized Plaintext)...\")\n",
        "for i in range(0,10):\n",
        "    X_train, X_test, y_train, y_test = split(x,y)\n",
        "    model = LogisticRegression(C=1)\n",
        "    # Fit the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    start_time = time.time()\n",
        "    # Run the predictions on non-encrypted data as a reference\n",
        "    y_pred_clear = model.predict(X_test)\n",
        "    clear_recall = recall_score(y_test, y_pred_clear, average='weighted') * 100\n",
        "    quantized_results.append(f\"{clear_recall:.4f}\")\n",
        "    # print(\"Accuracy rate for quantized plaintext:  \", model.score(X_test,y_test) * 100, \"%\")\n",
        "    # print(f\"Running time is {time.time() - start_time} seconds\")\n",
        "\n",
        "for r in quantized_results:\n",
        "    print(f\"{r}\")\n",
        "\n",
        "fhe_results = []\n",
        "print(\"Getting performance metrics for Concrete-ML model (FHE)...\")\n",
        "for i in range(0,10):\n",
        "    X_train, X_test, y_train, y_test = split(x,y)\n",
        "    model = LogisticRegression(C=1)\n",
        "    #Fit the model\n",
        "    model.fit(X_train, y_train)\n",
        "    #print(\"Compiling...\")\n",
        "    model.compile(x)\n",
        "    #print(\"model compiled!\")\n",
        "    y_pred_fhe = model.predict(X_test, fhe=\"execute\")\n",
        "    fhe_recall = recall_score(y_test, y_pred_fhe, average='weighted') * 100\n",
        "    fhe_results.append(f\"{fhe_recall:.4f}\")\n",
        "\n",
        "\n",
        "for r in fhe_results:\n",
        "    print(f\"{r}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get running time for training of models\n",
        "print(\"Getting training time and number of iterations for scikit-learn model (Plaintext)...\")\n",
        "results = []\n",
        "\n",
        "for i in range(0,10):\n",
        "    start_time = time.time()\n",
        "    X_train, X_test, y_train, y_test = split(x,y)\n",
        "    skmodel = skLR(C=1)\n",
        "    skmodel.fit(X_train,y_train)\n",
        "    results.append(f\"{time.time() - start_time}\")\n",
        "\n",
        "for r in results:\n",
        "    print(f\"{r}\")\n",
        "\n",
        "print(\"Getting training time and number of iterations for Concrete-ML model (Quantized Plaintext and FHE)...\")\n",
        "quantized_results = []\n",
        "\n",
        "for i in range(0,10):\n",
        "    X_train, X_test, y_train, y_test = split(x,y)\n",
        "    model = LogisticRegression(C=1)\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    quantized_results.append(f\"{time.time() - start_time}\")\n",
        "\n",
        "for r in quantized_results:\n",
        "    print(f\"{r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Getting running time for scikit-learn model (Plaintext)...\")\n",
        "results = []\n",
        "\n",
        "for i in range(0,10):\n",
        "    X_train, X_test, y_train, y_test = split(x,y)\n",
        "    skmodel = skLR(C=1)\n",
        "    skmodel.fit(X_train,y_train)\n",
        "    start_time = time.time()\n",
        "    y_pred_sklearn = skmodel.predict(X_test)\n",
        "    results.append(f\"{time.time() - start_time}\")\n",
        "\n",
        "for r in results:\n",
        "    print(f\"{r}\")\n",
        "\n",
        "print(\"Getting running time for Concrete-ML model (Quantized Plaintext)...\")\n",
        "quantized_results = []\n",
        "\n",
        "for i in range(0,10):\n",
        "    X_train, X_test, y_train, y_test = split(x,y)\n",
        "    model = LogisticRegression(C=1)\n",
        "    model.fit(X_train, y_train)\n",
        "    start_time = time.time()\n",
        "    y_pred_clear = model.predict(X_test)\n",
        "    quantized_results.append(f\"{time.time() - start_time}\")\n",
        "\n",
        "for r in quantized_results:\n",
        "    print(f\"{r}\")\n",
        "\n",
        "print(\"Getting running time for Concrete-ML model (FHE)...\")\n",
        "fhe_results = []\n",
        "\n",
        "for i in range(0,10):\n",
        "    X_train, X_test, y_train, y_test = split(x,y)\n",
        "    model = LogisticRegression(C=1)\n",
        "    model.fit(X_train, y_train)\n",
        "    model.compile(x)\n",
        "    start_time = time.time()\n",
        "    y_pred_clear = model.predict(X_test)\n",
        "    fhe_results.append(f\"{time.time() - start_time}\")\n",
        "\n",
        "for r in fhe_results:\n",
        "    print(f\"{r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiuG5jkrGI4a",
        "outputId": "5fc4db53-2181-4b24-a675-ce4586ea2863"
      },
      "outputs": [],
      "source": [
        "# # Initialize model and fix the number of bits to used for quantization \n",
        "\n",
        "# model = LogisticRegression(C=1)\n",
        "\n",
        "# # Fit the model\n",
        "# model.fit(X_train, y_train)\n",
        "\n",
        "# start_time = time.time()\n",
        "# # Run the predictions on non-encrypted data as a reference\n",
        "# y_pred_clear = model.predict(X_test)\n",
        "\n",
        "# # Output (plaintext vs FHE):\n",
        "# # print(\"In clear:  \", le.inverse_transform(y_pred_clear))\n",
        "# # accuracy_score(y_test, y_pred_clear)\n",
        "# print(\"Accuracy rate for quantized plaintext:  \", model.score(X_test,y_test) * 100, \"%\")\n",
        "# print(f\"Running time is {time.time() - start_time} seconds\")\n",
        "\n",
        "# #low/higly volatile accuracy may be attributed to small dataset and too many features \n",
        "# #(feature selection needed and more samples are required)\n",
        "# print(\"PREDICTION:\\n\", y_pred_clear)\n",
        "# print(\"ACTUAL:\\n\", y_test) \n",
        "\n",
        "# # Compile into a FHE model\n",
        "# start_time = time.time()\n",
        "# print(\"Compiling...\")\n",
        "# model.compile(x)\n",
        "# print(\"model compiled!\")\n",
        "# print(f\"Running time is {time.time() - start_time} seconds\")\n",
        "\n",
        "# # Run the inference in FHE\n",
        "# start_time = time.time()\n",
        "# print(\"Predicting in FHE...\")\n",
        "# y_pred_fhe = model.predict(X_test, fhe=\"execute\")\n",
        "# print(\"Accuracy rate for FHE:  \", accuracy_score(y_test, y_pred_fhe) * 100, \"%\")\n",
        "\n",
        "# #print(\"In FHE    :\", y_pred_fhe)\n",
        "# print(f\"Comparison: {int((y_pred_fhe == y_pred_clear).sum()/len(y_pred_fhe)*100)}% similar\")\n",
        "# print(f\"Running time is {time.time() - start_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAnw8wOaGKbT",
        "outputId": "7ec60758-6657-4088-aa98-26af90c2a1ea"
      },
      "outputs": [],
      "source": [
        "#Get AUC for multiclass\n",
        "#NOTE WE HAVE A MULTICLASS BUT NOT MULTILABEL PROBLEM. only one label selected from multiple classes is assigned\n",
        "start_time = time.time()\n",
        "results = []\n",
        "quantized_results = []\n",
        "fhe_results = []\n",
        "\n",
        "print(\"Getting performance metrics for scikit-learn model (Plaintext)...\")\n",
        "for i in range(0,10):\n",
        "    X_train, X_test, y_train, y_test = split(x,y)\n",
        "    #start_time = time.time()\n",
        "    skmodel = skLR(C=1)\n",
        "    skmodel.fit(X_train,y_train)\n",
        "    results.append(roc_auc_score(y, skmodel.predict_proba(x), multi_class='ovr'))\n",
        "\n",
        "for r in results:\n",
        "    print(f\"{r}\")\n",
        "\n",
        "print(\"Getting performance metrics for Concrete-ML model (Quantized Plaintext)...\")\n",
        "for i in range(0,10):\n",
        "    X_train, X_test, y_train, y_test = split(x,y)\n",
        "    model = LogisticRegression(C=1)\n",
        "    # Fit the model\n",
        "    model.fit(X_train, y_train)\n",
        "    quantized_results.append(roc_auc_score(y, model.predict_proba(x), multi_class='ovr'))\n",
        "\n",
        "for r in quantized_results:\n",
        "    print(f\"{r}\")\n",
        "\n",
        "print(\"Getting performance metrics for Concrete-ML model (FHE)...\")\n",
        "for i in range(0,10):\n",
        "    X_train, X_test, y_train, y_test = split(x,y)\n",
        "    model = LogisticRegression(C=1)\n",
        "    # Fit the model\n",
        "    model.fit(X_train, y_train)\n",
        "    #print(\"Compiling...\")\n",
        "    model.compile(x)\n",
        "    fhe_results.append(roc_auc_score(y, model.predict_proba(x), multi_class='ovr'))\n",
        "\n",
        "for r in fhe_results:\n",
        "    print(f\"{r}\")\n",
        "\n",
        "#print(\"Scikit learn ROC AUC Score (One vs Rest): \", roc_auc_score(y, skmodel.predict_proba(x), multi_class='ovr'))\n",
        "#print(\"Scikit learn AUROC (One vs One): \", roc_auc_score(y, skmodel.predict_proba(x), multi_class='ovo'))\n",
        "#print(\"Macro-averaged Concrete-ML ROC AUC Score (One vs Rest): \", roc_auc_score(y, model.predict_proba(x), multi_class='ovr'))\n",
        "#print(\"Concrete-ML AUROC (One vs One): \", roc_auc_score(y, model.predict_proba(x), multi_class='ovo'))\n",
        "#print(f\"Running time is {time.time() - start_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwU0x9zZIF9E",
        "outputId": "4d155d33-5969-4250-d551-d27adff6f678"
      },
      "outputs": [],
      "source": [
        "# In[10]:\n",
        "start_time = time.time()\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "print(\"***Note: The diagonal elements are the correctly predicted samples. ***\")\n",
        "\n",
        "#use the scikit-learn classes because we use the same class naman\n",
        "X_train, X_test, y_train, y_test = split(x,y)\n",
        "\n",
        "print(\"Confusion matrix for SKLearn Plaintext: \")\n",
        "#print(confusion_matrix(y_test, y_pred_sklearn), \"\\n\")\n",
        "skmodel = skLR(C=1)\n",
        "skmodel.fit(X_train,y_train)\n",
        "y_pred_sklearn = skmodel.predict(X_test)\n",
        "sklearn_cm_display = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_sklearn), display_labels=le.classes_)\n",
        "sklearn_cm_display.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"Confusion matrix for Quantized Plaintext: \")\n",
        "#print(confusion_matrix(y_test, y_pred_clear), \"\\n\")\n",
        "model = LogisticRegression(C=1)\n",
        "model.fit(X_train,y_train)\n",
        "y_pred_clear = model.predict(X_test)\n",
        "concrete_plain_display = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_clear), display_labels=le.classes_)\n",
        "concrete_plain_display.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"Confusion matrix for FHE: \")\n",
        "#print(confusion_matrix(y_test, y_pred_fhe))\n",
        "model.compile(x)\n",
        "y_pred_fhe = model.predict(X_test)\n",
        "concrete_fhe_display = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_fhe), display_labels=le.classes_)\n",
        "concrete_fhe_display.plot()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Running time is {time.time() - start_time} seconds\")\n",
        "\n",
        "print(f\"Comparison (FHE vs Plaintext): {int((y_pred_fhe == y_pred_sklearn).sum()/len(y_pred_fhe)*100)}% similar\")\n",
        "print(f\"Comparison (FHE vs Quantized Plaintext): {int((y_pred_fhe == y_pred_clear).sum()/len(y_pred_fhe)*100)}% similar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhNGYU2NGL6r",
        "outputId": "ea2a01fa-4d3c-433f-955a-d7d66e520ac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sklearn Accuracy (Linear Reg, RF, SVC):\n",
            "95.25288058915982 99.43788645306351 98.65092748735245\n",
            "95.29271264209858 99.49409780775717 98.98819561551433\n",
            "95.31075311412462 99.55030916245082 99.32546374367622\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m#print(\"Sklearn Random Forest Accuracy: \",skmodel3.score(X_test,y_test)*100,\"%\")\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[39m#SKLEARN SVC\u001b[39;00m\n\u001b[1;32m     24\u001b[0m skmodel4 \u001b[39m=\u001b[39m skSVC()\n\u001b[0;32m---> 25\u001b[0m skmodel4\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     26\u001b[0m skmodel4\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m     27\u001b[0m \u001b[39m#print(\"Sklearn SVC Accuracy: \",skmodel4.score(X_test,y_test)*100,\"%\")\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:257\u001b[0m, in \u001b[0;36mLinearSVC.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    254\u001b[0m check_classification_targets(y)\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n\u001b[0;32m--> 257\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_, n_iter_ \u001b[39m=\u001b[39m _fit_liblinear(\n\u001b[1;32m    258\u001b[0m     X,\n\u001b[1;32m    259\u001b[0m     y,\n\u001b[1;32m    260\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC,\n\u001b[1;32m    261\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[1;32m    262\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintercept_scaling,\n\u001b[1;32m    263\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m    264\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpenalty,\n\u001b[1;32m    265\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdual,\n\u001b[1;32m    266\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    267\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m    268\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m    269\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[1;32m    270\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmulti_class,\n\u001b[1;32m    271\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss,\n\u001b[1;32m    272\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    273\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[39m# Backward compatibility: _fit_liblinear is used both by LinearSVC/R\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[39m# and LogisticRegression but LogisticRegression sets a structured\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[39m# `n_iter_` attribute with information about the underlying OvR fits\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m# while LinearSVC/R only reports the maximum value.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m n_iter_\u001b[39m.\u001b[39mmax()\u001b[39m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1205\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m   1202\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m   1204\u001b[0m solver_type \u001b[39m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[0;32m-> 1205\u001b[0m raw_coef_, n_iter_ \u001b[39m=\u001b[39m liblinear\u001b[39m.\u001b[39;49mtrain_wrap(\n\u001b[1;32m   1206\u001b[0m     X,\n\u001b[1;32m   1207\u001b[0m     y_ind,\n\u001b[1;32m   1208\u001b[0m     sp\u001b[39m.\u001b[39;49misspmatrix(X),\n\u001b[1;32m   1209\u001b[0m     solver_type,\n\u001b[1;32m   1210\u001b[0m     tol,\n\u001b[1;32m   1211\u001b[0m     bias,\n\u001b[1;32m   1212\u001b[0m     C,\n\u001b[1;32m   1213\u001b[0m     class_weight_,\n\u001b[1;32m   1214\u001b[0m     max_iter,\n\u001b[1;32m   1215\u001b[0m     rnd\u001b[39m.\u001b[39;49mrandint(np\u001b[39m.\u001b[39;49miinfo(\u001b[39m\"\u001b[39;49m\u001b[39mi\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mmax),\n\u001b[1;32m   1216\u001b[0m     epsilon,\n\u001b[1;32m   1217\u001b[0m     sample_weight,\n\u001b[1;32m   1218\u001b[0m )\n\u001b[1;32m   1219\u001b[0m \u001b[39m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \u001b[39m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m \u001b[39m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m \u001b[39m# srand supports\u001b[39;00m\n\u001b[1;32m   1223\u001b[0m n_iter_max \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(n_iter_)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from concrete.ml.sklearn.svm import LinearSVC\n",
        "from sklearn.svm import LinearSVC as skSVC\n",
        "from concrete.ml.sklearn.rf import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier as skRF\n",
        "\n",
        "print(f\"Sklearn Accuracy (Linear Reg, RF, SVC):\")\n",
        "\n",
        "for i in range(0,10):\n",
        "    X_train, X_test, y_train, y_test = split(x,y)\n",
        "\n",
        "    #SKLEARN LINEAR REGRESSION\n",
        "    skmodel2 = skLinear()\n",
        "    skmodel2.fit(X_train,y_train)\n",
        "    skmodel2.predict(X_test)\n",
        "    #print(\"Sklearn Linear Regression Accuracy: \", skmodel2.score(X_test,y_test)*100,\"%\")\n",
        "\n",
        "    #SKLEARN RANDOM FOREST\n",
        "    skmodel3 = skRF()\n",
        "    skmodel3.fit(X_train, y_train)\n",
        "    skmodel3.predict(X_test)\n",
        "    #print(\"Sklearn Random Forest Accuracy: \",skmodel3.score(X_test,y_test)*100,\"%\")\n",
        "\n",
        "    #SKLEARN SVC\n",
        "    skmodel4 = skSVC()\n",
        "    skmodel4.fit(X_train, y_train)\n",
        "    skmodel4.predict(X_test)\n",
        "    #print(\"Sklearn SVC Accuracy: \",skmodel4.score(X_test,y_test)*100,\"%\")\n",
        "\n",
        "    print(f\"{skmodel2.score(X_test,y_test)*100} {skmodel3.score(X_test,y_test)*100} {skmodel4.score(X_test,y_test)*100}\")\n",
        "\n",
        "print(f\"Concrete-ML Accuracy (Linear Reg, RF, SVC):\")\n",
        "\n",
        "for i in range(0,10):\n",
        "    X_train, X_test, y_train, y_test = split(x,y)\n",
        "    model2 = LinearRegression()\n",
        "    model2.fit(X_train,y_train)\n",
        "    model2.predict(X_test)\n",
        "    #print(\"Concrete-ML Linear Regression Accuracy: \", model2.score(X_test,y_test)*100,\"%\")\n",
        "\n",
        "    model3 = RandomForestClassifier()\n",
        "    model3.fit(X_train, y_train)\n",
        "    model3.predict(X_test)\n",
        "    #print(\"Concrete-ML Random Forest Accuracy: \",model3.score(X_test,y_test)*100,\"%\")\n",
        "\n",
        "    model4 = LinearSVC()\n",
        "    model4.fit(X_train, y_train)\n",
        "    model4.predict(X_test)\n",
        "    #print(\"Concrete-ML SVC Accuracy: \",model4.score(X_test,y_test)*100,\"%\")\n",
        "\n",
        "    print(f\"{model2.score(X_test,y_test)*100}   {model3.score(X_test,y_test)*100}   {model4.score(X_test,y_test)*100}\")\n",
        "\n",
        "print(f\"Concrete-ML FHE Accuracy (Linear Reg, RF, SVC):\")\n",
        "\n",
        "for i in range(0,10):\n",
        "    print(f\"Run {i+1}:\")\n",
        "    X_train, X_test, y_train, y_test = split(x,y)\n",
        "    model2 = LinearRegression()\n",
        "    model2.fit(X_train,y_train)\n",
        "    model2.predict(X_test)\n",
        "    #print(\"Concrete-ML Linear Regression Accuracy: \", model2.score(X_test,y_test)*100,\"%\")\n",
        "\n",
        "    model3 = RandomForestClassifier()\n",
        "    model3.fit(X_train, y_train)\n",
        "    model3.predict(X_test)\n",
        "    #print(\"Concrete-ML Random Forest Accuracy: \",model3.score(X_test,y_test)*100,\"%\")\n",
        "\n",
        "    model4 = LinearSVC()\n",
        "    model4.fit(X_train, y_train)\n",
        "    model4.predict(X_test)\n",
        "    #print(\"Concrete-ML SVC Accuracy: \",model4.score(X_test,y_test)*100,\"%\")\n",
        "\n",
        "    print(f\"{model2.score(X_test,y_test)*100}   {model3.score(X_test,y_test)*100}   {model4.score(X_test,y_test)*100}\")\n",
        "\n",
        "    model2.compile(x)\n",
        "    model3.compile(x)\n",
        "    model4.compile(x)\n",
        "    model2.predict(X_test)\n",
        "    model3.predict(X_test)\n",
        "    model4.predict(X_test)\n",
        "\n",
        "    print(f\"FHE Version:\")\n",
        "    print(f\"{model2.score(X_test,y_test)*100}   {model3.score(X_test,y_test)*100}   {model4.score(X_test,y_test)*100}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#testing 10 runs to see the average compilation time of the FHE model\n",
        "fhe_compile_results = []\n",
        "\n",
        "print(\"Getting compilation metrics in seconds for Concrete-ML model (FHE)...\")\n",
        "for i in range(0,10):\n",
        "    X_train, X_test, y_train, y_test = split(x,y)\n",
        "    model = LogisticRegression(C=1)\n",
        "    # Fit the model\n",
        "    model.fit(X_train, y_train)\n",
        "    #print(\"Compiling...\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    model.compile(x)\n",
        "    fhe_compile_results.append(f\"{time.time()-start_time}\")\n",
        "\n",
        "for r in fhe_compile_results:\n",
        "    print(f\"{r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYE6bmsjGj4C",
        "outputId": "ab4bb57a-e5aa-4022-9870-8bc63d5bc978"
      },
      "outputs": [],
      "source": [
        "# In[10]:\n",
        "\n",
        "\n",
        "#Attempting to save the model\n",
        "from concrete.ml.deployment import FHEModelClient, FHEModelDev, FHEModelServer\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# fhemodel_dev = FHEModelDev(\"./concrete-covid-classifier\", model)\n",
        "# fhemodel_dev.save()\n",
        "\n",
        "print(f\"Running time for saving the FHE model is {time.time() - start_time} seconds\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "numpy.save('classes.npy', le.classes_)\n",
        "\n",
        "print(f\"Running time for saving the LabelEncoder classes is {time.time() - start_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kAxGRM1Glz1",
        "outputId": "6576cdbd-2aeb-454b-9d8f-a59cf9b5e09b"
      },
      "outputs": [],
      "source": [
        "#Saving Scikit-learn model\n",
        "from joblib import dump, load\n",
        "\n",
        "dump(skmodel, './scikit-learn-classifier/scikitlearnmodel.joblib')\n",
        "#loaded_model = load('scikitlearnmodel.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#trying condition numbers\n",
        "import numpy as np\n",
        "\n",
        "X_train, X_test, y_train, y_test = split(x,y)\n",
        "\n",
        "skmodel = skLR(C=1)\n",
        "skmodel.fit(X_train,y_train)\n",
        "sklearn_condition = np.linalg.cond(skmodel.predict_proba(X_test))\n",
        "\n",
        "model = LogisticRegression(C=1)\n",
        "model.fit(X_train, y_train)\n",
        "quant_condition = np.linalg.cond(model.predict_proba(X_test))\n",
        "\n",
        "model.compile(x)\n",
        "fhe_condition = np.linalg.cond(model.predict_proba(X_test))\n",
        "\n",
        "print(f\"Condition number of scikit-learn model:\\n{sklearn_condition}\")\n",
        "print(f\"Condition number of quantized model:\\n{quant_condition}\")\n",
        "print(f\"Condition number of FHE model:\\n{fhe_condition}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
